---
title: "InvestigationOfSampleSizePlanningInPsycSci"
author: "Felix Singleton Thorn"
date: "30 September 2018"
output: word_document
---

```{r echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(MultinomialCI)

# Wilson binomial CI - adapted from https://www.r-bloggers.com/reference-chart-for-precision-of-wilson-binomial-proportion-confidence-interval/
# n is the absolute number of paricipants, p is the proportion
WilsonBinCI <-  function(n, p, a=0.05) {
  z <- qnorm(1-a/2,lower.tail=FALSE)
  l <- 1/(1+1/n*z^2)*(p + 1/2/n*z^2 + 
                        z*sqrt(1/n*p*(1-p) + 1/4/n^2*z^2))
  u <- 1/(1+1/n*z^2)*(p + 1/2/n*z^2 -
                        z*sqrt(1/n*p*(1-p) + 1/4/n^2*z^2))
list(estimate = p, lower=l, upper=u)
}

data <- readxl::read_excel("PowerAnalysesPsycSci.xlsx")

# Data cleaning
data$Justificaiton[data$Justificaiton == "Conservative estimate"] <- "Informal assertion of effect size"
data$Justificaiton[data$Justificaiton == "Rule of thumb"] <- NA
data$Justificaiton[data$Justificaiton == "Informal assertion that this will be sufficent"] <- NA
data$Justificaiton[data$Justificaiton == "Based sample on single previous study's sample size (no PA)"] <- NA
data$Justificaiton[data$Justificaiton == "Average effect size in a set of studies (not a formal meta-analysis), reduced for publication bias in an ad hoc manner"] <- "Average effect size in a set of studies (not a formal meta-analysis), reduced for publication bias"
data$Justificaiton[data$Justificaiton == "Post hoc (i.e., justification of obtained sample size)"] <- "Sensitivity analysis"

tabJust<-table(data$Justificaiton)
tabJustForCIS<-table(c(data$Justificaiton, rep("No power analysis reported", sum(is.na(data$Justificaiton)))))

tabCIs <- data.frame(tabJustForCIS,round(as.numeric(tabJustForCIS)/nrow(data)*100), round(multinomialCI(tabJustForCIS, alpha = .05)*100))
names(tabCIs) <- c("Effect size selection method", 'n', 'Percent', '95_CI_LB', '95_CI_UB')

percent <- function(n,N = nrow(data)) {
  percent <- round(n/N * 100)
  print(paste0(percent, "%"))
}

# wilson CIs on proportion reporting a PA 
CIonProp <- round(as.numeric(WilsonBinCI(nrow(data), sum(data$PowerAnalysis)/nrow(data))) * 100)
# 
```


In order to get an initial estimate of the research planning practices common in psychology I assessed the `r nrow(data)` empirical research articles published in the November 2017 to August 2018 issues of Psychological Science. The sample size was determined in order to constrain multinomial confidence interval width to a maximum of 20%. 

Of the `r nrow(data)` empirical research articles published during this period `r sum(data$PowerAnalysis==1)` articles reported a power analysis,  `r CIonProp[1]`% of sampled articles, Wilson score interval [`r paste0(CIonProp[2], "%, ", CIonProp[3], "%")`]. Of the reported power analyses, the most common approach was to effect size selection was to use a `r names(tabJust[tabJust==max(tabJust)])` as the effect size, with `r max(tabJust)` articles ( `r tabCIs$Percent[tabCIs[1] == names(tabJust[tabJust==max(tabJust)])]` % of articles) reporting having done so. 

Despite the fact that pilot studies are almost by definition too small to reliably estimate the true population parameter value of interest, `r tabCIs$n[ tabCIs[1] == "Pilot study"]` studies reported having estimated the effect size with this value `r tabCIs$Percent[ tabCIs[1] == "Pilot study"]`% of articles.  


Almost as many used benchmarks from Cohen (1988; n = `r sum(tabCIs$n[grepl(pattern = "Cohen", tabCIs[,1])])`, `r round((sum(tabCIs$n[grepl(pattern = "Cohen", tabCIs[,1])])/nrow(data))*100)` % of articles). Other articles either reported a sensitivity analysis (showing the effect size that the sample size gave them 80% power to detect, n =  `r tabCIs$n[ tabCIs[1] == "Sensitivity analysis"]`  (`r tabCIs$Percent[ tabCIs[1] == "Sensitivity analysis"]`% of articles)  in order to justify the obtained sample size. `r sum(tabCIs$n[grepl(pattern = "Informal assertion of effect size", tabCIs[,1])])`, `r round((sum(tabCIs$n[grepl(pattern = "Informal assertion of effect size", tabCIs[,1])])/nrow(data))*100)`% of articles, did not provide any justification for the effect size they reported having used in power analysis, and `r sum(tabCIs$n[grepl(pattern = "No effect size stated", tabCIs[,1])])` articles (`r round((sum(tabCIs$n[grepl(pattern = "No effect size stated", tabCIs[,1])])/nrow(data))*100)`% of all articles surveyed) did not state the effect size that they used in a reported power analysis.

Just `r sum(tabCIs$n[grepl(pattern = "Lowest effect size reported in a previous paper on this topic", tabCIs[,1]) | grepl(pattern = ", reduced for publication bias", tabCIs[,1]) ])`, articles, `r round((sum(tabCIs$n[grepl(pattern = "Lowest effect size reported in a previous paper on this topic", tabCIs[,1]) | grepl(pattern = " reduced for publication bias", tabCIs[,1]) ])/nrow(data))*100)`% of the examined articles, reported that they adjusted their estimates for publication bias, and all of these articles used ad-hoc methods such as doubling the sample size that resulted from a power analysis or using the lowest reported effect for an intervention as opposed to the more sophisticated methods that have been proposed.

 
Table [1] The number and percentage of papers reporting each type of justification for the effect sizes reported in their power analysis.

```{r, echo=FALSE}
kable(tabCIs[order(tabCIs$n, decreasing = T),]) 
examples <- data.frame("Category" = as.character(tabCIs$`Effect size selection method`),"Example" = as.character(tabCIs$`Effect size selection method`), stringsAsFactors = F)

for(i in 1:length(examples$Category)) { 
  if(sum(grepl(as.character(examples$Category[i]), data$Justificaiton, fixed = T))>=1)
   examples[i,2] <- data$TextFromPaper[grepl(as.character(examples$Category[i]), data$Justificaiton, fixed = T)][sample(sum(grepl(examples$Category[i], data$Justificaiton, fixed = T)), size = 1)]}

```
 

The sample is not representative of psychology articles in general, but does give an....


`r kable(examples)`

